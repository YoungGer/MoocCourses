makeVideo(teamName1="JoelDan",teamName2="SarahMax",
trueLabels=battleData[battleSample,1],team1Pred=nonPCA[battleSample,"JoelDan"],
team2Pred=nonPCA[battleSample,"SarahMax"],testImages=as.matrix(battleData[battleSample,-1]))
makeVideo(teamName1="GregColin",teamName2="SarahMax",
trueLabels=battleData[battleSample,1],team1Pred=nonPCA[battleSample,"GregColin"],
team2Pred=nonPCA[battleSample,"SarahMax"],testImages=as.matrix(battleData[battleSample,-1]))
makeVideo(teamName1="GregColin",teamName2="JoelDan",
trueLabels=battleData[battleSample,1],team1Pred=nonPCA[battleSample,"GregColin"],
team2Pred=nonPCA[battleSample,"JoelDan"],testImages=as.matrix(battleData[battleSample,-1]))
names(PCA)
battleSample = sample(1:3000,size=100,replace=FALSE)
makeVideo(teamName1="GregColinPCA",teamName2="JonPhoebe",
trueLabels=battleData[battleSample,1],team1Pred=PCA[battleSample,"GregColinPCA"],
team2Pred=PCA[battleSample,"JonPhoebe"],testImages=as.matrix(battleData[battleSample,-1]))
head(PCA)
names(PCA)
battleSample = sample(1:3000,size=100,replace=FALSE)
makeVideo(teamName1="WesJoeEm",teamName2="CalebChey",
trueLabels=battleData[battleSample,1],team1Pred=PCA[battleSample,"WesJoeEm"],
team2Pred=PCA[battleSample,"CalebChey"],testImages=as.matrix(battleData[battleSample,-1]))
battleSample = sample(1:3000,size=100,replace=FALSE)
makeVideo(teamName1="JonPhoebe",teamName2="GregColin",
trueLabels=battleData[battleSample,1],team1Pred=PCA[battleSample,"JonPhoebe"],
team2Pred=nonPCA[battleSample,"GregColin"],testImages=as.matrix(battleData[battleSample,-1]))
displayTeams <- function(teamName1="The Jets",teamName2="The Sharks",cx=5,nFrames=10){
par(mar=c(0,0,4,0),bg="black",col.main="white")
layout(matrix(c(1,1,1,2,2,2),byrow=T,nrow=2))
colRampWhite = colorRampPalette(c("black","white"))(nFrames)
colRampRed= colorRampPalette(c("black","red"))(nFrames)
colRampBlue= colorRampPalette(c("black","blue"))(nFrames)
for(i in 1:nFrames){
Team1
plot(1:10,1:10,type="n")
text(5.5,5.5,teamName1,col = colRampRed[i], family = 'Helvetica', cex = cx)
Team2
plot(1:10,1:10,type="n",main="Versus",cex.main=3)
text(5.5,5.5,teamName2,col = colRampBlue[i], family = 'Helvetica', cex = cx)
Sys.sleep(2/nFrames)
}
predictionMatch <- function(teamName1,teamName2,trueLabels,testImages,team1Pred,team2Pred,cx=8){
par(mar=c(0,0,4,0),bg="black",col.main="white")
layout(matrix(c(2,1,4,3,0,5),byrow=T,nrow=2))
for(i in 1:dim(testImages)[1]){
zz = matrix(testImages[i,],nrow=28)
image(zz[,ncol(zz):1],main="True Image",xaxt="n",yaxt="n",col=colRampWhite,cex.main=3)
Sys.sleep(0.2)
plot(1:10,1:10,type="n",xaxt="n",yaxt="n",main=paste(teamName1),cex.main=3)
text(5.5,5.5,team1Pred[i],col ="red", family = 'Helvetica', cex = cx)
plot(1:10,1:10,type="n",xaxt="n",yaxt="n",main=paste("Accuracy"),cex.main=3)
text(5.5,5.5,paste(cumMean(trueLabels[1:i] == team1Pred[1:i]),"%"),col ="red", family = 'Helvetica', cex = cx/1.5)
Sys.sleep(0.2)
plot(1:10,1:10,type="n",xaxt="n",yaxt="n",main=paste(teamName2),cex.main=3)
text(5.5,5.5,team2Pred[i],col ="blue", family = 'Helvetica', cex = cx)
plot(1:10,1:10,type="n",xaxt="n",yaxt="n",main=paste("Accuracy"),cex.main=3)
text(5.5,5.5,paste(cumMean(trueLabels[1:i] == team2Pred[1:i]),"%"),col ="blue", family = 'Helvetica', cex = cx/1.5)
Sys.sleep(0.4)
}
par(mar=c(0,0,4,0),bg="black",col.main="white")
layout(matrix(c(1,1,1,2,0,3),byrow=T,nrow=2))
plot(1:10,1:10,type="n",xaxt="n",yaxt="n",main="Winner",cex.main=3)
team1winner = cumMean(trueLabels ==team1Pred) > cumMean(trueLabels==team2Pred)
if(team1winner){
text(5.5,5.5,teamName1,col ="red", family = 'Helvetica', cex = cx/1.5)
}else{
text(5.5,5.5,teamName2,col ="blue", family = 'Helvetica', cex = cx/1.5)
}
plot(1:10,1:10,type="n",xaxt="n",yaxt="n",main=paste(teamName1),cex.main=3)
text(5.5,5.5,paste(cumMean(trueLabels[1:i] == team1Pred[1:i]),"%"),col ="red", family = 'Helvetica', cex = cx/1.5)
plot(1:10,1:10,type="n",xaxt="n",yaxt="n",main=paste(teamName2),cex.main=3)
text(5.5,5.5,paste(cumMean(trueLabels[1:i] == team2Pred[1:i]),"%"),col ="blue", family = 'Helvetica', cex = cx/1.5)
}
cumMean = function(x){
return(round(mean(x)*100,1))
}
makeVideo <- function(teamName1,teamName2,trueLabels,team1Pred,team2Pred,testImages){
displayTeams(teamName1=teamName1,teamName2=teamName2)
predictionMatch(teamName1=teamName1,teamName2=teamName2,
trueLabels=trueLabels,team1Pred=team1Pred,
team2Pred=team2Pred,testImages=testImages)
}
nonPCA = read.csv("~/Dropbox/Leah/Teaching/SM439/SM439/Project2/nonPCAbattle.csv",header=T)
PCA = read.csv("~/Dropbox/Leah/Teaching/SM439/SM439/Project2/PCAbattle.csv",header=T)
battleData= read.csv("~/Dropbox/Leah/Teaching/SM439/SM439/Project2/battleData.csv",header=T)
battleSample = sample(1:3000,size=5,replace=FALSE)
makeVideo(teamName1="JoelDan",teamName2="SarahMax",
trueLabels=battleData[battleSample,1],team1Pred=nonPCA[battleSample,"JoelDan"],
team2Pred=nonPCA[battleSample,"SarahMax"],testImages=as.matrix(battleData[battleSample,-1]))
battleSample = sample(1:3000,size=100,replace=FALSE)
makeVideo(teamName1="JoelDan",teamName2="SarahMax",
trueLabels=battleData[battleSample,1],team1Pred=nonPCA[battleSample,"JoelDan"],
team2Pred=nonPCA[battleSample,"SarahMax"],testImages=as.matrix(battleData[battleSample,-1]))
windowsFonts()
?par
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}#
## These come from the paper Pubmed ID = 20876667 and are reports from## a GWAS. Update some P-values that are incorrectly scraped because of## variation in scientific notation.pvalueData[pvalueData[,1] == 10,1] = 10e-7pvalueData = pvalueData[-1,]## These come from the Lancet, with missed periods in the P-valuespvalueData[13392,1] = 0.003pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4
dim(pvalueDAta)
dim(pvalueData)
pvalueData2 = pvalueData
load("~/Downloads/pvalueData00-10.rda")
ls()
dim(pvalueData)
pvalueData[1,]
pvalueData2[1,]
names(pvalueDAta)
names(pvalueData)
names(pvalueData2)
dim(pvalueData)
dim(pvalueData2)
pvalueData2[1,]
pvalueData2[2,]
pvalueData2[1,]
pvalueData[1,]
oo = match(pvalueData$pubmedID,pvalueData2$pubmedID)
colnames(pvalueData)
colnames(pvalueData2)
oo = match(pvalueData[,3],pvalueData2[,3])
length(oo)
oo[1]
sum(is.na(oo))
tmp1 = pvalueData[,3]
tmp2 = pvalueData2[,3]
tmp1[1]
tmp2[1]
tmp2 = as.numeric(tmp2)
tmp1 = tmp1[order(tmp1)]
tmp2 = tmp2[order(tmp2)]
tmp1[1]
tmp2[1]
length(tmp1)
length(tmp2)
sum(tmp1[1:15653] == tmp2[1:15653])
tmp1[13427:13428]
tmp1[13427:13429]
tmp2[13427:13429]
tmp1[13425:13429]
tmp2[13425:13429]
pvalueData[1,]
pvalueData[pvalueData[,3]==19549972, ]
pvalueData2[pvalueData2[,3]=="19549972", ]
pvalueData2[pvalueData2[,3]=="19549973", ]
tmp2[13425:13429]
tmp2[13424:13429]
tmp1[13424:13429]
sum(tmp1[1:13424] == tmp2[1:13424])
sum(tmp1[1:11940] == tmp2[1:11940])
plot(tmp1,tmp2)
length(tmp2)
plot(tmp1,tmp2[1:15653])
t1 = table(tmp1)
t2 = table(tmp2)
t1[1]
t2[1]
sum(t1 == t2)
length(t2)
length(t1)
setdiff(names(t1),names(t2))
sum(t1 == t2[1:5323])
oo = match(names(t1),names(t2))
length(oo)
oo[1]
oo = match(names(t2),names(t1))
length(oo)
sum(is.na(oo))
which(is.na(oo))
length(names(t2))
t2[3097]
sum(t2)
dim(pvalueData)
dim(pvalueData2)
pvalueData2[pvalueData2[,3]=="16421237",]
pvalueData3 = pvalueData2[-which(pvalueData2[,3]=="16421237"),]
dim(pvalueData3)
dim(pvalueData)
tmp1 = pvalueData3[,3]
tmp2 = pvalueData[,3]
length(tmp1)
length(tmp2)
tmp1[1]
tmp2[1]
tmp1 = as.numeric(tmp1)
tmp1 = tmp1[order(tmp1)]
tmp2 = tmp2[order(tmp2)]
tmp1[1]
tmp2[1]
plot(tmp1,tmp2)
sum(tmp1==tmp2)
mean(tmp1==tmp2)
tmp1 = pvalueData3[,3]#
tmp2 = pvalueData[,3]
tmp1[1]
tmp1 = as.numeric(tmp1)
colnames(pvalueData3)
pvals1 = as.numeric(pvalueData3[,1])
pvals2 = as.numeric(pvalueData[,1])
pvals1[1]
pvals2[1]
pvals1 = pvals1[order(tmp1)]
pvals2 = pvals1[order(tmp2)]
sum(pvals1==pvals2)
length(pvals1)
ne = which(pvals1!=pvals2)
ne[1]
pvals1 = as.numeric(pvalueData3[,1])
pvals2 = as.numeric(pvalueData[,1])
pvals1 = pvals1[order(tmp1)]
pvals2 = pvals2[order(tmp2)]
sum(pvals1==pvals2)
length(pvals1)
mean(pvals1==pvals2)
mean((pvals1 - pvals2) < 1e-3)
mean((pvals1 - pvals2) < 1e-5)
mean(abs(pvals1 - pvals2) < 1e-5 )
dim(pvalueData)
dim(pvalueData3)
rm(list=ls())
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}#
## These come from the paper Pubmed ID = 20876667 and are reports from## a GWAS. Update some P-values that are incorrectly scraped because of## variation in scientific notation.pvalueData[pvalueData[,1] == 10,1] = 10e-7pvalueData = pvalueData[-1,]## These come from the Lancet, with missed periods in the P-valuespvalueData[13392,1] = 0.003pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4# This one has a <<< in the P-value definitionpvalueData = pvalueData[-which(pvalueData[,3]=="16421237"),] #
save(pvalueData,npapers,file="pvalueDataTest.rda")
dim(pvalueData)
rm(list=ls())
pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4# This one has a <<< in the P-value definitionpvalueData = pvalueData[-which(pvalueData[,3]=="16421237"),] #
save(pvalueData,npapers,file="pvalueDataTest.rda")
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}#
## These come from the paper Pubmed ID = 20876667 and are reports from## a GWAS. Update some P-values that are incorrectly scraped because of## variation in scientific notation.pvalueData[pvalueData[,1] == 10,1] = 10e-7pvalueData = pvalueData[-1,]## These come from the Lancet, with missed periods in the P-valuespvalueData[13392,1] = 0.003pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4# This one has a <<< in the P-value definitionpvalueData = pvalueData[-which(pvalueData[,3]=="16421237"),] #
save(pvalueData,npapers,file="pvalueDataTest.rda")
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}#
## These come from the paper Pubmed ID = 20876667 and are reports from## a GWAS. Update some P-values that are incorrectly scraped because of## variation in scientific notation.pvalueData[pvalueData[,1] == 10,1] = 10e-7pvalueData = pvalueData[-1,]## These come from the Lancet, with missed periods in the P-valuespvalueData[13392,1] = 0.003pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4# This one has a <<< in the P-value definitionpvalueData = pvalueData[-which(pvalueData[,3]=="16421237"),] #
save(pvalueData,npapers,file="pvalueData.rda")
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}#
## These come from the paper Pubmed ID = 20876667 and are reports from## a GWAS. Update some P-values that are incorrectly scraped because of## variation in scientific notation.pvalueData[pvalueData[,1] == 10,1] = 10e-7pvalueData = pvalueData[-1,]## These come from the Lancet, with missed periods in the P-valuespvalueData[13392,1] = 0.003pvalueData[13413,1] = 0.014pvalueData[13414,1] = 0.004## This one comes from the Lancet too, where an incorrect P-value is grabbed.pvalueData = pvalueData[-14674,]#
# Remove rows with P-values that are pvalueData = pvalueData[!is.na(pvalueData[,1]),]# This one for some reason replaced a period with "small middle dot"pvalueData[which(pvalueData[,3] == 11943262),1] = 1e-4# This one has a <<< in the P-value definitionpvalueData = pvalueData[-which(pvalueData[,3]=="16421237"),] #
save(pvalueData,npapers,file="pvalueData.rda")
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1
]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]
?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}#
## Run the above functions over a specified set of journals and years## to obtain P-values#
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}
Functions to scrape P-values from Pubmed abstracts# Date: 7-1-12# Copyright (C) 2011 Jeffrey T. Leek (http://www.biostat.jhsph.edu/~jleek/contact.html) and Leah R. Jager##    This program is free software: you can redistribute it and/or modify#    it under the terms of the GNU General Public License as published by#    the Free Software Foundation, either version 3 of the License, or#    (at your option) any later version.##    This program is distributed in the hope that it will be useful,#    but WITHOUT ANY WARRANTY; without even the implied warranty of#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the#    GNU General Public License for more details, see <http://www.gnu.org/licenses/>.###    Note: These functions were written on a Mac and may have difficulties when#          read on windows machines. ##########################################################################################
library(RCurl)library(XML)library(tm)#
# A function to get all abstracts and pubmed ids for papers from the journal "journaltitle" in the year "year"# by scraping the Pubmed API.getAbstractsPmids = function(journaltitle,year){# esearchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"q = paste("db=pubmed&term=",gsub(" ","+",journaltitle),"[ta]+AND+",year,"[dp]&usehistory=y",sep="")esearch <- xmlTreeParse(getURL(paste(url, q, sep="")), useInternal = T)webenv  <- xmlValue(getNodeSet(esearch, "//WebEnv")[[1]])key     <- xmlValue(getNodeSet(esearch, "//QueryKey")[[1]])# efetchurl <- "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"q   <- "db=pubmed&retmode=xml&rettype=abstract"efetch <- xmlTreeParse(getURL(paste(url, q, "&WebEnv=", webenv, "&query_key=", key, sep="")), useInternal = T)r = xmlRoot(efetch)n = xmlSize(r)abstracts = pmid = titles = rep(NA,n)for(i in 1:n){abstracts[i] =  xmlValue(r[[i]][[1]][["Article"]][["Abstract"]]); pmid[i] = xmlValue(r[[i]][[1]][["PMID"]]); titles[i] = xmlValue(r[[i]][[1]][["Article"]][["ArticleTitle"]]) }return(list(abstracts=abstracts,pmid=pmid,titles=titles))}#
#A function to remove trailing zeros from the P-value stringsremoveTrailing = function(string){  while(length(grep("[0-9]",strsplit(string,"")[[1]][nchar(string)])) == 0){    string = substr(string,1,(nchar(string)-1))  }  return(string)}#
# A function to convert the scientific notation used by journals into# numeric values that can be analyzed. convertScientific = function(string){  if(length(grep("[[:punct:]][[:space:]]",string))>0){    string = strsplit(string,"[[:punct:]][[:space:]]")[[1]][1]    string = removeTrailing(string)  }  if(length(grep("[×x]",string))>0){    string = gsub("[:space:]","",string)    tmp1 = as.numeric(strsplit(string,"[×x]")[[1]][1])    tmp2 = as.numeric(strsplit(strsplit(string,"[×x]")[[1]][2],"[−-]")[[1]][2])    return(tmp1*10^(-tmp2))  }else{    return(as.numeric(string))  }}#
# A function to scrape the P-values from a vector of abstracts with corresponding# pubmed idsgetPvalues = function(abstract,pmid){  pvalues = numeric(0)  trunc = numeric(0)  ids = numeric(0)  # Get the truncated p-values  ind = grep("[Pp][[:space:]]?[<≤]",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?[<≤]")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,1)        ids = c(ids,pmid[ind[i]])      }    }  }   # Get the truncated p-values  ind = grep("[Pp][[:space:]]?=",abstract)  for(i in 1:length(ind)){    tmp = strsplit(abstract[ind[i]],"[[:space:](][Pp][[:space:]]?=")[[1]]    n = length(tmp)    for(j in 1:n){      if(length(grep("[.0123456789]",substr(tmp[j],1,2))) > 0){        if(length(grep("[A-Z]",substr(tmp[j],1,1)))>0){next;}        tmp2 = strsplit(tmp[j],"[^[:punct:][:space:][:digit:]x[:space:]]")[[1]][1]        tmp2 = removeTrailing(tmp2)        tmp2 = gsub(" ","",tmp2)        tmp2 = convertScientific(tmp2)        pvalues = c(pvalues,as.numeric(tmp2))        trunc = c(trunc,0)        ids = c(ids,pmid[ind[i]])      }    }  }  return(list(pvalues=pvalues,ids=ids,trunc=trunc))}
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}
tmpData
i
j
tmpData = getAbstractsPmids(journals[i],years[j])
while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")
journals = c("JAMA","New England Journal of Medicine","BMJ","American Journal of Epidemiology","Lancet")years = 2000:2010pvalueData = matrix(NA,nrow=1,ncol=6)colnames(pvalueData) = c("pvalue","pvalueTruncated","pubmedID","year","abstract","title")npapers = matrix(NA,nrow=length(journals),ncol=length(years))for(i in 1:length(journals)){  for(j in 1:length(years)){    cat(journals[i]); cat(" "); cat(years[j]); cat(" ");     tmpData = getAbstractsPmids(journals[i],years[j])    while(length(tmpData$abstracts) ==1 & is.na(tmpData$abstracts[1])){tmpData = getAbstractsPmids(journals[i],years[j])}    cat("Downloaded"); cat(" ");    npapers[i,j] = length(tmpData$abstracts)    tmpOut = getPvalues(tmpData$abstracts,tmpData$pmid)#
nPvalues = length(tmpOut$pvalues)    aa = match(tmpOut$ids,tmpData$pmid)#
tmpMatrix = cbind(tmpOut$pvalues,tmpOut$trunc,as.numeric(tmpOut$ids),rep(years[j],nPvalues),tmpData$abstracts[aa],tmpData$titles[aa])    rownames(tmpMatrix) = rep(journals[i],nPvalues)    pvalueData = rbind(pvalueData,tmpMatrix)    cat("Done\n")  }}
barplot(c(1,1,-3))
?barplot
pres12 = read.csv("~/Desktop/2012-pres.csv")
pres12[1,]
pres08 = read.csv("~/Desktop/2008pres.csv")
pres08[1,]
sum(pres12$FIPS == 0)
table(pres12$X[pres12$FIPS==0,])
table(pres12$X[pres12$FIPS==0])
pres12[pres12$X=="OK",]
pres12 = pres12[pres12$FIPS==0,]
dim(pres12)
pres12[1,]
pres12[2,]
pres12 = read.csv("~/Desktop/2012-pres.csv")
pres12[1:10,]
pres12[21:30,]
pres12[50:100,]
pres12 = pres12[pres12$FIPS==0,]
length(unique(pres12$X))
which.max(pres12$X == "OK")
which(pres12$X == "OK")
pres12[c(36,37)]
pres12[c(36,37),]
pres12[1,]
pres12[1,5]
pres12[1,4]
as.numeric(pres12[1,4])
as.numeric(as.character(pres12[1,4])
)
as.numeric(as.character(pres12[1,4]))
pres08[1,]
sum(pres08$LAST.NAME == "McCain")
sum(pres08$LAST.NAME == "McCain" & pres08$STATE=="Alabama")
pres08[pres08$LAST.NAME == "McCain",]
pres12 = read.csv("~/Desktop/2012-pres.csv")
pres12 = pres12[pres12$FIPS==0,]
pres12[1,]
table(unique(pres12))
table(unique(pres12$x))
table(unique(pres12$X))
substr
gsub
gsub(pres12[1,3])
pres12[1,]
pres12$Obama.vote[1]
dem12 = as.character(pres12$Obama.vote)
dem12[1]
dem12 = sapply(dem12,function(x){gsub(x,",","")})
dem12[1]
dem12 = as.character(pres12$Obama.vote)
gsub(dem12[1],",","")
?gsub
gsub(",","",dem12[1])
dem12 = as.character(pres12$Obama.vote)
dem12 = as.numeric(sapply(dem12,function(x){gsub(",","",x)}))
dem12[1]
hist(dem12)
pres12 = read.csv("~/Desktop/2012-pres.csv")
pres12 = pres12[pres12$FIPS==0,]
dem12 = as.character(pres12$Obama.vote)
dem12 = as.numeric(sapply(dem12,function(x){gsub(",","",x)}))
rep12 = as.character(pres12$Romney.vote)
rep12 = as.numeric(sapply(rep12,function(x){gsub(",","",x)}))
plot(rep12,dem12)
rep12[1]
pres12[1,]
table(pres12[,2])
res12 = read.csv("~/Desktop/2012-pres.csv")
pres12 = pres12[pres12$FIPS==0,]
table(pres12[,2])
table(pres12[,1])
library(XML)
theurl <- "http://uselectionatlas.org/RESULTS/data.php?year=2008&datatype=national&def=1&f=0&off=0&elect=0"
tables <- readHTMLTable(theurl)
tables[1,]
dim(tables)
tables[1]
tables
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
n.rows
library(slidify)
?author
setwd("~/Documents/Work/teaching/2013/coursera/")
ls()
list.files()
author("gettingStarted")
slidify("gettingStarted")
ls()
list.files()
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
setwd("~/Documents/Work/teaching/2013/coursera/")
setwd("gettingHelp/")
slidify("index.Rmd")
library(slidify)
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
getwd()
setwd("../aboutDataAnalysis/")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
setwd("~/Dropbox/Jeff/teaching/2013/")
setwd("753/lectures/")
setwd("001courseMotivation/")
library(slidify)
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
setwd("~/Dropbox/Jeff/teaching/2013/753/jhsph753/lectures/001courseMotivation/")
slidify("index.Rmd")
library(slidify)
slidify("index.Rmd")
browseURL("index.html")
